# MolBERT chemical language transformer

## Model Identifier
- Slug: molbert
- Ersilia ID: eos2thm
- Tags: fingerprint, ML, descriptor

## Model Description 
BERT language transformer applied to molecular structures 
- Input: SMILES 
- Output: Vector 
- Model type:
- Mode of training:
- Training Data: ~1,600,000 compounds 
- Experimentally Validated: No 

## Source code
This model is published by Benedek Fabian et al. Molecular representation learning with language models and domain-relevant auxiliary tasks. *arXivLabs* (2020). DOI: https://doi.org/10.48550/arXiv.2011.13230
- Code: https://github.com/BenevolentAI/MolBERT

## License
The GPL-v3 license applies to all parts of the repository that are not externally maintained libraries. This repository uses the externally maintained library "MolBERT", located at `/model` and licensed under a MIT License

## History 
- Model was downloaded and incorporated on September 28, 2021

